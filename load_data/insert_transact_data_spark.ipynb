{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql import DataFrame\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, MapType, ArrayType,BooleanType\n",
        "# from graphframes import *\n",
        "\n",
        "f_uuid = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
        "f_bool = F.udf(lambda: True, BooleanType())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create mount point for reading and saving data\n",
        "# Note linked-storage-service is register in Synapse\n",
        "mssparkutils.fs.mount( \n",
        "    \"abfss://bronze@xxxxxxx.dfs.core.windows.net\", \n",
        "    \"/mydata\", \n",
        "    {\"linkedService\":\"linked-storage-service\"} \n",
        ")\n",
        "job_id = mssparkutils.env.getJobId()\n",
        "bronze_mount_point = f\"synfs:/{job_id}/mydata/archive.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "cosmosEndpoint = \"https://ebcbin5oofjcs.documents.azure.com:443/\" # note use document endpoint and not gremlin endpoint\n",
        "cosmosMasterKey = \"xxxxxxxxxxxxxxxx\"\n",
        "cosmosDatabaseName = \"database01\"\n",
        "cosmosContainerName = \"graph01\" # \"/accountId as partitionKey\"\n",
        "\n",
        "cfg = {\n",
        "  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\" : cosmosMasterKey,\n",
        "  \"spark.cosmos.database\" : cosmosDatabaseName,\n",
        "  \"spark.cosmos.container\" : cosmosContainerName,\n",
        "}\n",
        "# Configure Catalog Api to be used\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", cosmosEndpoint)\n",
        "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey\", cosmosMasterKey)\n",
        "spark.conf.set(\"spark.cosmos.throughputControl.enabled\",True)\n",
        "spark.conf.set(\"spark.cosmos.throughputControl.targetThroughput\",20000)\n",
        "\n",
        "def write_to_cosmos_graph(df: DataFrame, data_type: str, save: bool = False):\n",
        "    if (save):\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"synfs:/{job_id}/mydata/{data_type}/\")\n",
        "        \n",
        "    df.write\\\n",
        "   .format(\"cosmos.oltp\")\\\n",
        "   .options(**cfg)\\\n",
        "   .mode(\"APPEND\")\\\n",
        "   .save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Build dataframe to injest assuming \"accountId\" as partition key for the graph01 cosmos collection\n",
        "def prep_cosmos_vertices_df(df: DataFrame) -> DataFrame:\n",
        "    tmp = df.select(\"id\")\n",
        "    return tmp.withColumn('label',F.lit('account')).withColumn('accountId',df['id']).\\\n",
        "    select(\"label\",\"id\",\"accountId\").distinct()\n",
        "\n",
        "# Create vertices based on activity and num of transaction made from raw data\n",
        "def prepare_vertices_sample(df: DataFrame) -> DataFrame:\n",
        "    nameOrig = df.select(\"nameOrig\")\n",
        "    nameDest = df.select(\"nameDest\")\n",
        "    busy_accounts = nameOrig.union(nameDest).withColumnRenamed('nameOrig','id')\n",
        "    top_account_activity = busy_accounts.groupBy('id').count().filter(F.col('count') > 10 ).select('id') # accounts with more than 10 transactions\n",
        "    top_account_amount = df.filter(F.col('amount') > 1000000) # accounts with amount transfer more than 1000000\n",
        "    nameOrigA = top_account_amount.select('nameOrig')\n",
        "    nameDestA = top_account_amount.select('nameDest')\n",
        "    top_account_transact = nameOrigA.union(nameDestA).withColumnRenamed('nameOrig','id').distinct().select('id')\n",
        "    vertices = top_account_activity.union(top_account_transact).distinct()\n",
        "    return prep_cosmos_vertices_df(vertices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def prep_cosmos_edges_df(df: DataFrame) -> DataFrame:\n",
        "   return df.select(\"type\",\"amount\",\"nameOrig\",\"nameDest\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\").\\\n",
        "    withColumn(\"label\",df['type']).\\\n",
        "    withColumn('accountId',df['nameOrig']).\\\n",
        "    withColumnRenamed('nameOrig','_vertexId').\\\n",
        "    withColumn('_sinkPartition',df['nameDest']).\\\n",
        "    withColumnRenamed('nameDest',\"_sink\").\\\n",
        "    withColumn('_sinkLabel',F.lit(\"account\")).\\\n",
        "    withColumn('_vertexLabel',F.lit(\"account\")).\\\n",
        "    withColumn('_isEdge',f_bool()).\\\n",
        "    withColumn('id', f_uuid()).\\\n",
        "    select(\"id\",\"label\",\"_sink\",\"_sinkLabel\",\"_sinkPartition\",\"_vertexId\",\"_vertexLabel\",\"_isEdge\",\"accountId\",\"type\",\"amount\",\"oldbalanceOrg\",\"oldbalanceDest\",\"newbalanceDest\").\\\n",
        "    distinct()\n",
        "\n",
        "\n",
        "# Filter transaction that have either the source or destination as vertices\n",
        "def prepare_edges_sample_1(df: DataFrame, vertices:DataFrame) -> DataFrame:\n",
        "    transactions_nameOrig = df.join(vertices, df['nameOrig'] == vertices['id'])\n",
        "    transactions_nameDest = df.join(vertices, df['nameDest'] == vertices['id'])\n",
        "    edges = transactions_nameOrig.union(transactions_nameDest).select(\"type\",\"amount\",\"nameOrig\",\"nameDest\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\")\n",
        "    return prep_cosmos_edges_df(edges)\n",
        "    \n",
        "    \n",
        "\n",
        "# Filter transaction that have both source and destination as vertices\n",
        "def prepare_edges_sample_2(df: DataFrame, vertices:DataFrame) -> DataFrame:\n",
        "    transactions = df.join(\n",
        "        vertices, df.nameOrig == vertices.id)\\\n",
        "        .drop(vertices.id)\\\n",
        "        .join(\n",
        "            vertices.alias(\"c\"), df.nameDest == vertices.id)\n",
        "    return prep_cosmos_edges_df(transactions)\n",
        "\n",
        "# Ingest all edges\n",
        "def prepare_edges_all(df:DataFrame) -> DataFrame:\n",
        "    return prep_cosmos_edges_df(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def all_vertices(df: DataFrame) -> DataFrame:\n",
        "    nameOrig = df.select(\"nameOrig\")\n",
        "    nameDest = df.select(\"nameDest\")\n",
        "    tmp = nameOrig.union(nameDest).withColumnRenamed('nameOrig','id')\n",
        "    return prep_cosmos_vertices_df(tmp)\n",
        "\n",
        "def all_edges(df:DataFrame) -> DataFrame:\n",
        "    tmp = raw_data.select(\"type\",\"amount\",\"nameOrig\",\"nameDest\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\")\n",
        "    return prep_cosmos_edges_df(tmp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def injest_sample_data_1() -> None:\n",
        "    sample_transact = raw_data.limit(50000)\n",
        "    v = all_vertices(sample_transact)\n",
        "    e = prepare_edges_sample_2(sample_transact,v)\n",
        "    write_to_cosmos_graph(v,\"vertices\")\n",
        "    write_to_cosmos_graph(e,\"edges\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Ingest the entire dataset\n",
        "# edge_mount = f\"synfs:/{job_id}/mydata/edges\"\n",
        "# vertices_mount = f\"synfs:/{job_id}/mydata/vertices\"\n",
        "raw_data = spark.read.format('delta').load(bronze_mount_point).distinct()\n",
        "v = all_vertices(raw_data)\n",
        "e = prepare_edges_sample_2(raw_data,v) \n",
        "write_to_cosmos_graph(v,\"vertices\",True)\n",
        "write_to_cosmos_graph(e,\"edges\",True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# # Create test data set to verify vertices and edges are being created correctly\n",
        "# cosmos_edges_df_test = cosmos_edges_df.limit(10)\n",
        "# source = cosmos_edges_df_test.select('_vertexId')\n",
        "# dest = cosmos_edges_df_test.select('_sink')\n",
        "# tmp = source.union(dest).distinct().withColumnRenamed('_vertexId','id').withColumn('label',F.lit('account'))\n",
        "# cosmos_vertices_df_test = tmp.withColumn('accountId',tmp['id']).select(\"label\",\"id\",\"accountId\")\n",
        "# print(f\"Number of Accounts/Vertices: {cosmos_vertices_df_test.count()}, Number of transactions/Edges: {cosmos_edges_df_test.count()}\")\n",
        "# write_to_cosmos_graph(cosmos_vertices_df_test,\"vertices\",False)\n",
        "# write_to_cosmos_graph(cosmos_edges_df.\"edges\",False)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "interpreter": {
      "hash": "f9cd62efb0fc51286d3613ac05078f54464dadbae5b8bf8b1358fb34bf40e623"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
